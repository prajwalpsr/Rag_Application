# ğŸš€ Simple RAG Application
Qdrant â€¢ LlamaIndex â€¢ Ollama â€¢ FastAPI â€¢ Inngest â€¢ Streamlit â€¢ uv

This project is a fully open-source Retrieval-Augmented Generation (RAG) application that runs 100% locally.

## ğŸ“¦ Features
- Document ingestion (PDF, TXT, MD)
- LlamaIndex chunking + embedding
- Qdrant vector storage
- Retrieval-Augmented Generation
- Local LLM via Ollama
- Background jobs using Inngest
- FastAPI backend
- Streamlit UI
- Fully offline

## ğŸ§° New System Setup
### Install Python
https://www.python.org/downloads/

### Install uv
pip install uv

### Install Ollama
https://ollama.com/download  
ollama pull llama3

### Install Docker (for Qdrant)
https://www.docker.com/products/docker-desktop/

### Install Dependencies
uv sync  
or  
pip install -r requirements.txt

## ğŸš€ Run Application (4 Terminals)
### Terminal 1: Start Qdrant
Windows:
docker run -p 6333:6333 -v "${pwd}/qdrant_storage:/qdrant/storage" qdrant/qdrant

Mac/Linux:
docker run -p 6333:6333 -v "$(pwd)/qdrant_storage:/qdrant/storage" qdrant/qdrant

### Terminal 2: Start FastAPI
uv run uvicorn main:app --reload  
or  
uvicorn main:app --reload

### Terminal 3: Start Inngest
npx inngest-cli@latest dev -u http://127.0.0.1:8000/api/inngest --no-discovery

### Terminal 4: Start Streamlit
uv run streamlit run streamlit_ui.py  
or  
streamlit run streamlit_ui.py

## ğŸ“¥ Ingest Documents
curl -X POST http://127.0.0.1:8000/ingest

## ğŸ” Query Example
curl -X POST http://127.0.0.1:8000/query -H "Content-Type: application/json" -d '{"query":"What is this project about?", "top_k":5}'

## ğŸ“‚ Project Structure
Rag_Application/
â”‚â”€â”€ main.py  
â”‚â”€â”€ streamlit_ui.py  
â”‚â”€â”€ inngest_workflows/  
â”‚â”€â”€ qdrant_storage/  
â”‚â”€â”€ data/  
â”‚â”€â”€ pyproject.toml  
â”‚â”€â”€ requirements.txt  
â”‚â”€â”€ README.md

## ğŸ”’ .env Example
QDRANT_URL=http://localhost:6333  
LLM_MODEL=llama3  
EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
